{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data from Redshift using the boto3 Redshift data api of AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input the SQL query to be run <span style=\"color:red\">*(Change the SQL query with yours)*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # SQL query without the comments\n",
    "# simple example\n",
    "query = \"select * from <schema.table> limit 10;\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters for the Redshift data API  <span style=\"color:green\">*(No changes neeeded in this section)*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters passed from the caller event\n",
    "# cluster identifier for the Amazon Redshift cluster\n",
    "redshift_cluster_id = \"<clustername>\"\n",
    "# database name for the Amazon Redshift cluster\n",
    "redshift_database_name = \"<dbname>\"\n",
    "# database user in the Amazon Redshift cluster with access to execute relevant SQL queries\n",
    "redshift_user = \"<username>\"\n",
    "# IAM Role of Amazon Redshift cluster having access to S3\n",
    "redshift_iam_role = \"arn:aws:iam::<iam role>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define run_sql function using Redshift Data API to get query results into pandas dataframe  <span style=\"color:green\">*(No changes neeeded in this section)*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "\n",
    "def run_sql(sql_text):\n",
    "    # initiate redshift-data redshift_data_api_client in boto3\n",
    "    redshift_data_api_client = boto3.client('redshift-data')\n",
    "    \n",
    "    # Initiate the query\n",
    "    res = redshift_data_api_client.execute_statement(\n",
    "            Database=redshift_database_name, DbUser=redshift_user, Sql=sql_text, ClusterIdentifier=redshift_cluster_id)\n",
    "    \n",
    "    \n",
    "    query_id = res[\"Id\"]\n",
    "    done = False\n",
    "    while not done:\n",
    "        time.sleep(1)\n",
    "        status_description = redshift_data_api_client.describe_statement(Id=query_id)\n",
    "        status = status_description[\"Status\"]\n",
    "        if status == \"FAILED\":\n",
    "            raise Exception('SQL query failed:' + query_id + \": \" + status_description[\"Error\"])\n",
    "        elif status == \"FINISHED\":\n",
    "            if status_description['ResultRows']>0:\n",
    "                results = redshift_data_api_client.get_statement_result(Id=query_id)\n",
    "                column_labels = []\n",
    "                for i in range(len(results[\"ColumnMetadata\"])): column_labels.append(results[\"ColumnMetadata\"][i]['label'])\n",
    "                records = []\n",
    "                for record in results.get('Records'):\n",
    "                    records.append([list(rec.values())[0] for rec in record])\n",
    "                df = pd.DataFrame(np.array(records), columns=column_labels)\n",
    "                return df\n",
    "            else:\n",
    "                return query_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your queries and save the results in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_sql(query);\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/blob/main/quick-start/python/RedShiftDataAPI.py\n",
    "2. https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/blob/main/use-cases/sagemaker-notebook-redshift-ml/redshift_ml_with_data_api.ipynb\n",
    "3. https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html#data-api-calling-considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "##### Alternate way of accessing Redshift using psycopg2 library, gives a timeout error.  However using the redshift data api is a good approach, as it uses the boto3 library from AWS and data can be accessed by IAM role instead of explicitly specifying the password"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
